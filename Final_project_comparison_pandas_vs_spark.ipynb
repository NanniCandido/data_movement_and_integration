{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fad6573",
   "metadata": {},
   "source": [
    "To compare time and memory consumption when processing CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95865045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, max, min, length\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa746c0",
   "metadata": {},
   "source": [
    "SMALL DATASET<br>\n",
    "Dataset: 'Nutrition_Physical_Activity_and_Obesity.csv'<br>\n",
    "Rows: 104,272<br>\n",
    "Columns: 33<br>\n",
    "Size: 34,934Kb<br>\n",
    "<br>\n",
    "Data.gov. (2023, December 8). U.S. Department of Health & Human Services - Nutrition, Physical Activity, and Obesity - Behavioral Risk Factor Surveillance System. Retrieved December 1, 2024, from https://catalog.data.gov/dataset/nutrition-physical-activity-and-obesity-behavioral-risk-factor-surveillance-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494da52",
   "metadata": {},
   "source": [
    "Monitor PANDAS processing (Small dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c786a362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PANDAS TIME AND MEMORY STATS: START => (2024-12-05 - 21:02:28)\n",
      "\n",
      "Pandas: Data loaded: 0.35 seconds, Memory: 27.90 MB\n",
      "Pandas: Filter operation: 0.12 seconds, Memory: 40.62 MB\n",
      "Pandas: Aggregation: 0.00 seconds, Memory: 0.10 MB\n",
      "\n",
      "PANDAS TIME AND MEMORY STATS: END => (2024-12-05 - 21:02:29)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To set the processing start\n",
    "print(f\"PANDAS TIME AND MEMORY STATS: START => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")\n",
    "\n",
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process=psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss/1024/1024\n",
    "\n",
    "# To track memory before loading\n",
    "mem_before_load=get_memory_usage()\n",
    "\n",
    "# To load the dataset\n",
    "start_time=time.time()\n",
    "df=pd.read_csv(\"nutrition-physical-activity.csv\")\n",
    "load_time=time.time()-start_time\n",
    "mem_after_load = get_memory_usage()\n",
    "\n",
    "# To process for-loop to create 1000000-size list\n",
    "start_time=time.time()\n",
    "new_list = []\n",
    "range_size = range(1, 1000001)\n",
    "for n in range_size:\n",
    "    new_list.insert(n, n*2)\n",
    "filter_time=time.time()-start_time\n",
    "mem_after_filter=get_memory_usage()\n",
    "\n",
    "# To calculate the range of Data_Value\n",
    "start_time=time.time()\n",
    "max_value=df['Data_Value'].max()\n",
    "min_value=df['Data_Value'].min()\n",
    "avg_value=df['Data_Value'].mean()\n",
    "range_value=max_value - min_value\n",
    "agg_time=time.time()-start_time\n",
    "mem_after_agg=get_memory_usage()\n",
    "\n",
    "# To print results\n",
    "print(f\"Pandas: Data loaded: {load_time:.2f} seconds, Memory: {mem_after_load-mem_before_load:.2f} MB\")\n",
    "print(f\"Pandas: Filter operation: {filter_time:.2f} seconds, Memory: {mem_after_filter-mem_after_load:.2f} MB\")\n",
    "print(f\"Pandas: Aggregation: {agg_time:.2f} seconds, Memory: {mem_after_agg-mem_after_filter:.2f} MB\\n\")\n",
    "\n",
    "print(f\"PANDAS TIME AND MEMORY STATS: END => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a6c3c",
   "metadata": {},
   "source": [
    "Monitor SPARK processing (Small dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbee1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK TIME AND MEMORY STATS: START => (2024-12-05 - 21:02:37)\n",
      "\n",
      "Spark: Data loaded: 10.20 seconds, Memory: -26.24 MB\n",
      "Spark: Filter operation: 0.16 seconds, Memory: 2.86 MB\n",
      "Spark: Aggregation: 1.90 seconds, Memory: 0.46 MB\n",
      "\n",
      "SPARK TIME AND MEMORY STATS: END => (2024-12-05 - 21:03:14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To set the processing start\n",
    "print(f\"SPARK TIME AND MEMORY STATS: START => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")\n",
    "\n",
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process=psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "# To start Spark session\n",
    "spark = SparkSession.builder.appName(\"Spark Memory Monitor\").getOrCreate()\n",
    "\n",
    "# To track memory before loading\n",
    "mem_before_load = get_memory_usage()\n",
    "\n",
    "# To load the dataset\n",
    "start_time = time.time()\n",
    "df = spark.read.csv(\"nutrition-physical-activity.csv\", header=True, inferSchema=True)\n",
    "load_time = time.time() - start_time\n",
    "mem_after_load = get_memory_usage()\n",
    "\n",
    "df_filtered = df.filter(col(\"Data_Value\").isNotNull())\n",
    "\n",
    "# To process for-loop to create 1,000,000-size list\n",
    "start_time=time.time()\n",
    "new_list = []\n",
    "range_size = range(1, 1000001)\n",
    "for n in range_size:\n",
    "    new_list.insert(n, n*2)  \n",
    "filter_time=time.time()-start_time\n",
    "mem_after_filter=get_memory_usage()\n",
    "\n",
    "# To calculate the range of Data_Value\n",
    "start_time=time.time()\n",
    "agg_values = df_filtered.agg(\n",
    "    max(\"Data_Value\").alias(\"max_value\"),\n",
    "    min(\"Data_Value\").alias(\"min_value\"),\n",
    "    avg(\"Data_Value\").alias(\"avg_value\")\n",
    ").collect()[0]\n",
    "range_value=agg_values.max_value - agg_values.min_value\n",
    "agg_time=time.time()-start_time\n",
    "mem_after_agg=get_memory_usage()\n",
    "\n",
    "# To print results\n",
    "print(f\"Spark: Data loaded: {load_time:.2f} seconds, Memory: {mem_after_load-mem_before_load:.2f} MB\")\n",
    "print(f\"Spark: Filter operation: {filter_time:.2f} seconds, Memory: {mem_after_filter-mem_after_load:.2f} MB\")\n",
    "print(f\"Spark: Aggregation: {agg_time:.2f} seconds, Memory: {mem_after_agg-mem_after_filter:.2f} MB\\n\")\n",
    "\n",
    "print(f\"SPARK TIME AND MEMORY STATS: END => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216d374",
   "metadata": {},
   "source": [
    "BIG DATASET<br>\n",
    "Dataset: 'yellow_tripdata_2015-01.csv'<br>\n",
    "Rows: 12,748.986<br>\n",
    "Columns: 19<br>\n",
    "Size: 1,939,419Kb<br>\n",
    "<br>\n",
    "https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07aea2b",
   "metadata": {},
   "source": [
    "Monitor PANDAS processing (Big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c66583c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PANDAS TIME AND MEMORY STATS: START => (2024-12-05 - 21:18:39)\n",
      "\n",
      "Pandas: Data loaded: 44.46 seconds, Memory: 2359.93 MB\n",
      "Pandas: Filter operation: 1.39 seconds, Memory: 393.59 MB\n",
      "Pandas: Aggregation: 0.08 seconds, Memory: 99.46 MB\n",
      "\n",
      "PANDAS TIME AND MEMORY STATS: END => (2024-12-05 - 21:19:25)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To set the processing start\n",
    "print(f\"PANDAS TIME AND MEMORY STATS: START => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")\n",
    "\n",
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process=psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss/1024/1024\n",
    "\n",
    "# To track memory before loading\n",
    "mem_before_load=get_memory_usage()\n",
    "\n",
    "# To load the dataset\n",
    "start_time=time.time()\n",
    "df=pd.read_csv(\"yellow_tripdata_2015-01.csv\", quotechar='\"') #, quoting=csv.QUOTE_MINIMAL)\n",
    "load_time=time.time()-start_time\n",
    "mem_after_load = get_memory_usage()\n",
    "\n",
    "# To process for-loop to create 10,000,000-size list\n",
    "start_time=time.time()\n",
    "new_list = []\n",
    "range_size = range(1, 10000001) # 10,000,000\n",
    "for n in range_size:\n",
    "    new_list.insert(n, n*2)\n",
    "filter_time=time.time()-start_time\n",
    "mem_after_filter=get_memory_usage()\n",
    "\n",
    "# To calculate the range of Data_Value\n",
    "start_time=time.time()\n",
    "# max_value=df['trip_distance'].str.len().max()\n",
    "# min_value=df['trip_distance'].str.len().min()\n",
    "max_value=df['trip_distance'].max()\n",
    "min_value=df['trip_distance'].min()\n",
    "range_value=max_value - min_value\n",
    "agg_time=time.time()-start_time\n",
    "mem_after_agg=get_memory_usage()\n",
    "\n",
    "# To print results\n",
    "print(f\"Pandas: Data loaded: {load_time:.2f} seconds, Memory: {mem_after_load-mem_before_load:.2f} MB\")\n",
    "print(f\"Pandas: Filter operation: {filter_time:.2f} seconds, Memory: {mem_after_filter-mem_after_load:.2f} MB\")\n",
    "print(f\"Pandas: Aggregation: {agg_time:.2f} seconds, Memory: {mem_after_agg-mem_after_filter:.2f} MB\\n\")\n",
    "\n",
    "print(f\"PANDAS TIME AND MEMORY STATS: END => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f087ab9",
   "metadata": {},
   "source": [
    "Monitor SPARK processing (Big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770b3439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK TIME AND MEMORY STATS: START => (2024-12-05 - 21:26:11)\n",
      "\n",
      "Spark: Data loaded: 21.07 seconds, Memory: -2567.06 MB\n",
      "Spark: Filter operation: 0.27 seconds, Memory: -406.32 MB\n",
      "Spark: Aggregation: 6.60 seconds, Memory: 3.35 MB\n",
      "\n",
      "SPARK TIME AND MEMORY STATS: END => (2024-12-05 - 21:26:40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To set the processing start\n",
    "print(f\"SPARK TIME AND MEMORY STATS: START => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")\n",
    "\n",
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process=psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "# To start Spark session\n",
    "spark = SparkSession.builder.appName(\"Spark Memory Monitor\").getOrCreate()\n",
    "\n",
    "# To track memory before loading\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "# To load the dataset\n",
    "start_time = time.time()\n",
    "df = spark.read.csv(\"yellow_tripdata_2015-01.csv\", quote='\"', header=True, inferSchema=True)\n",
    "load_time = time.time() - start_time\n",
    "mem_after_load = get_memory_usage()\n",
    "\n",
    "df_filtered = df.filter(col('trip_distance').isNotNull())\n",
    "\n",
    "# To process for-loop to create 1,000,000-size list\n",
    "start_time=time.time()\n",
    "new_list = []\n",
    "range_size = range(1, 1000001)\n",
    "for n in range_size:\n",
    "    new_list.insert(n, n*2)  \n",
    "filter_time=time.time()-start_time\n",
    "mem_after_filter=get_memory_usage()\n",
    "\n",
    "# To calculate the range of Data_Value\n",
    "start_time=time.time()\n",
    "agg_values = df_filtered.agg(\n",
    "    max('trip_distance').alias(\"max_value\"),\n",
    "    min('trip_distance').alias(\"min_value\")\n",
    ").collect()[0] \n",
    "range_value=agg_values.max_value - agg_values.min_value\n",
    "agg_time=time.time()-start_time\n",
    "mem_after_agg=get_memory_usage()\n",
    "\n",
    "# To print results\n",
    "print(f\"Spark: Data loaded: {load_time:.2f} seconds, Memory: {mem_after_load-mem_before:.2f} MB\")\n",
    "print(f\"Spark: Filter operation: {filter_time:.2f} seconds, Memory: {mem_after_filter-mem_after_load:.2f} MB\")\n",
    "print(f\"Spark: Aggregation: {agg_time:.2f} seconds, Memory: {mem_after_agg-mem_after_filter:.2f} MB\\n\")\n",
    "\n",
    "print(f\"SPARK TIME AND MEMORY STATS: END => ({date.today()} - {datetime.now().strftime('%H:%M:%S')})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
